
\section{Interval Arithmetic}
We can use rounding modes of floating point arithmetic to do rigorous computation to compute bounds on, for example, $\E$ or $\ensuremath{\pi}$. To do this we will use set/interval arithmetic which is then easily implemented on a computer. 

That is recall the set operations
\[
A + B = \{x + y : x \ensuremath{\in} A, y \ensuremath{\in} B\}, AB = \{xy : x \ensuremath{\in} A, y \ensuremath{\in} B\}, A/B = \{x/y : x \ensuremath{\in} A, y \ensuremath{\in} B\}
\]
We will use floating point arithmetic to construct approximate set operations \ensuremath{\oplus}, \ensuremath{\otimes} so that
\[
  A + B \ensuremath{\subseteq} A \ensuremath{\oplus} B, AB \ensuremath{\subseteq} A \ensuremath{\otimes} B, A/B \ensuremath{\subseteq} A \ensuremath{\oslash} B
\]
thereby a complicated algorithm can be run on sets and the true result is guaranteed to be a subset of the output. E.g. we can do $\ensuremath{\euler} = {\rm exp}(1) \ensuremath{\in} {\rm exp}([1,1]) \ensuremath{\subseteq} {\rm exp}^{\rm FP}([1,1])$ where ${\rm exp}^{\rm FP}$ is implemented using $\ensuremath{\oplus}$ and $\ensuremath{\otimes}$.

For intervals we can deduce formulas for basic arithmetic operations:

\begin{proposition}[interval bounds] For intervals  $A = [a,b]$ and $B = [c,d]$ such that $0 \ensuremath{\notin} A,B$ and integer $n \ensuremath{\neq} 0$, we have
\meeq{
A + B = [a+b, b+d] \ccr
A/n =  \begin{cases}
[a/n,b/n] & n > 0  \\
[b/n,a/n] & n < 0 
\end{cases} \ccr
AB = \begin{cases}
[ac, bd] & a, b, c, d \ensuremath{\geq} 0
\end{cases}
}
\end{proposition}
\textbf{Proof}

\ensuremath{\QED}

We want to  implement floating point variants of these operations. 

\begin{definition}[floating point interval arithmetic]  such that, for $S = [a,b] + [c,d]$  $P = [a,b] * [c,d]$, and $D = [a,b]/n$ for an integer $n$,


\begin{align*}
[a,b] \ensuremath{\oplus} [c,d] &:= [{\rm fl}^{\rm down}(a+b), {\rm fl}^{\rm up}(b+d)] \\
[a,b] \ensuremath{\oslash} n &:= [{\rm fl}^{\rm down}(\min D), {\rm fl}^{\rm up}(\max D)] \\
[a,b] \ensuremath{\otimes} [c,d] &:= [{\rm fl}^{\rm down}(\min P), {\rm fl}^{\rm up}(\max P)]
\end{align*}
\end{definition}

This guarantees $S \ensuremath{\subseteq} [a,b] \ensuremath{\oplus} [c,d]$, $P \ensuremath{\subseteq} [a,b] \ensuremath{\otimes} [c,d]$, and $D \ensuremath{\subseteq} [a,b] \ensuremath{\oslash} n$. In other words, if $x \ensuremath{\in} [a,b]$ and $y \ensuremath{\in} [c,d]$ then $x +y \ensuremath{\in} [a,b] \ensuremath{\oplus} [c,d]$, and we thereby have  bounds on $x + y$.

\begin{example}[exponential] As a motivating example, consider computing $\exp(x)$ from the Taylor series approximation:
\[
\exp(x) = \sum_{k=0}^{n-1} {x^k \over k!} + \ensuremath{\delta}
\]
If we can evaluate polynomials on intervals (eg., if we have access to \texttt{+} and \texttt{*} operations) then we can obtain a rigorous enclosure for the value of the polynomial on the right-hand side.  \end{example}



