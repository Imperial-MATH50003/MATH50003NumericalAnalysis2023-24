
\section{Structured Matrices}
We have seen how algebraic operations (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}) are defined exactly in terms of rounding ($\ensuremath{\oplus}$, $\ensuremath{\ominus}$, $\ensuremath{\otimes}$, $\ensuremath{\oslash}$)  for floating point numbers. Now we see how this allows us to do (approximate) linear algebra operations on matrices. 

A matrix can be stored in different formats, in particular we can take advantage of \emph{sparsity}: if we know a matrix has entries that are guaranteed to be zero we can implement faster algorithms. We shall see that this comes up naturally in numerical methods for solving differential equations.  In particular, we will discuss some basic matrices:

\begin{itemize}
\item[1. ] \emph{Dense}: This can be considered unstructured, where we need to store all entries in a vector or matrix. Matrix multiplication reduces directly to standard algebraic operations. Solving linear systems with dense matrices will be discussed later.


\item[2. ] \emph{Triangular}: If a matrix is upper or lower triangular, we can immediately invert using back-substitution. In practice we store a dense matrix and ignore the upper/lower entries.


\item[3. ] \emph{Banded}: If a matrix is zero apart from entries a fixed distance from  the diagonal it is called banded and this allows for more efficient algorithms. We discuss diagonal, tridiagonal and bidiagonal matrices.

\end{itemize}
\textbf{Remark} For those who took the first half of the module, there was an important emphasis on working with \emph{linear operators} rather than \emph{matrices}. That is, there was an emphasis on basis-independent mathematical techniques. However, in terms of practical computation we need to work with some representation of an operator and the  most natural is a matrix, a table of numbers. Hence we will only consider the case where we use the canonical basis. (This is also important as some of the students have not taken the first half of the module.)

\subsection{Dense matrices}
A basic operation is matrix-vector multiplication. For a field $\ensuremath{\bbF}$ (typically $\ensuremath{\bbR}$ or $\ensuremath{\bbC}$, or this can be relaxed to be a ring), $A \ensuremath{\in} \ensuremath{\bbF}^{m \ensuremath{\times} n}$ and $\ensuremath{\bm{\x}} \ensuremath{\in} \ensuremath{\bbF}^n$ we have the usual definition of matrix multiplication:
\[
A\ensuremath{\bm{\x}} := \begin{bmatrix} \ensuremath{\sum}_{j=1}^n a_{1,j} x_j \\ \ensuremath{\vdots} \\ \ensuremath{\sum}_{j=1}^n a_{m,j} x_j \end{bmatrix}.
\]
When we are working with floating point numbers $A \ensuremath{\in} F^{m \ensuremath{\times} n}$ we obtain an approximation:
\[
A\ensuremath{\bm{\x}} \ensuremath{\approx} \begin{bmatrix} \ensuremath{\bigoplus}_{j=1}^n (a_{1,j}  \ensuremath{\otimes} x_j) \\ \ensuremath{\vdots} \\  \ensuremath{\bigoplus}_{j=1}^n (a_{m,j}  \ensuremath{\otimes} x_j) \end{bmatrix}.
\]
This actually encodes an algorithm for computing the entries. And this algorithm uses $O(m n)$ floating point operations (FLOPs): each of the $m$ entry consists of $n$ multiplications and $n-1$ additions, hence we $2n-1 = O(n)$ per row for a total of $m(2n-1) = O(mn)$ operations. In the problem sheet we see how the floating point error can be bounded in terms of norms, thus reducing the problem to a purely mathematical concept.

Sometimes there are multiple ways of implementing numerical algorithms. We have an alternative formula where we multiple by columns: if we write
\[
A = \begin{bmatrix} \ensuremath{\bm{\a}}_1 | \ensuremath{\cdots} | \ensuremath{\bm{\a}}_n \end{bmatrix}
\]
where $\ensuremath{\bm{\a}}_j = A \ensuremath{\bm{\e}}_j \ensuremath{\in} \ensuremath{\bbF}^m$ being the columns of $A$ we also have
\[
A \ensuremath{\bm{\x}} = x_1 \ensuremath{\bm{\a}}_1  + \ensuremath{\cdots} + x_n \ensuremath{\bm{\a}}_n.
\]
The floating point formula for this is exactly the same as the previous algorithm and the number of operations is the same. Just the order of operations has changed.  Suprisingly, this latter version is significantly faster.

\subsection{Triangular matrices}
The simplest sparsity case is being triangular: where all entries above or below the diagonal are zero.  Matrix multiplication can be modified to take advantage of this. Eg., if $U \ensuremath{\in} \ensuremath{\bbF}^{n \ensuremath{\times} n}$ is upper triangular we have:
\[
U\ensuremath{\bm{\x}} = \begin{bmatrix} \ensuremath{\sum}_{j=1}^n a_{1,j} x_j \\ \ensuremath{\sum}_{j=2}^n a_{2,j} x_j  \\ \ensuremath{\vdots} \\ \ensuremath{\sum}_{j=n}^n a_{m,j} x_j \end{bmatrix}.
\]
When implemented in floating point this uses roughly half the number of multiplications: $n + (n-1) + \ensuremath{\ldots} + 1 = n(n+1)/2$ multiplications. The complexity is still $O(n^2)$ however. 

Triangularity allows us to also invert systems using forward- or back-substitution. In particular if $\ensuremath{\bm{\x}}$ solves $U \ensuremath{\bm{\x}} = \ensuremath{\bm{\b}}$ then we have:
\[
x_k = {b_k - \ensuremath{\sum}_{j=k+1}^n u_{kj} x_j \over u_{kk}}
\]
Thus we can compute $x_n, x_{n-1},\ensuremath{\ldots},x_1$ in sequence (using floating point operations).

\subsection{Banded matrices}
A \emph{banded matrix} is zero off a prescribed number of diagonals.  We call the number of (potentially) non-zero diagonals the \emph{bandwidths}:

\textbf{Definition (bandwidths)} A matrix $A$ has \emph{lower-bandwidth} $l$ if  $A[k,j] = 0$ for all $k-j > l$ and \emph{upper-bandwidth} $u$ if $A[k,j] = 0$ for all $j-k > u$. We say that it has \emph{strictly lower-bandwidth} $l$ if it has lower-bandwidth $l$ and there exists a $j$ such that $A[j+l,j] \neq 0$. We say that it has \emph{strictly upper-bandwidth} $u$ if it has upper-bandwidth $u$ and there exists a $k$ such that $A[k,k+u] \neq 0$.

A banded matrix has better complexity for matrix multiplication and solving linear systems:  we can multiple in $O(\min(m,n))$ operations. We consider two cases in particular (in addition to diagonal): bidiagonal and tridiagonal. 

\textbf{Definition (Bidiagonal)} If a square matrix has bandwidths $(l,u) = (1,0)$ it is \emph{lower-bidiagonal} and if it has bandwidths $(l,u) = (0,1)$ it is \emph{upper-bidiagonal}. 

For example, if
\[
U = \begin{bmatrix} u_{1,1} & u_{1,2} \\ & \ensuremath{\ddots} & \ensuremath{\ddots} \\
&&                                 u_{n-1,n-1} & u_{n-1,n} \\
&&& u_{n,n}
\end{bmatrix}
\]
is upper-bidiagonal multiplication becomes
\[
U\ensuremath{\bm{\x}} = \begin{bmatrix} u_{1,1} x_1 + u_{1,2} x_2 \\ u_{2,2} x_2 + u_{2,3} x_3  \\ \ensuremath{\vdots} \\ u_{n-1,n-1} x_{n-1} + u_{n-1,n} x_n \\u_{n,n} x_n \end{bmatrix}.
\]
This requires only $O(n)$ operations. A bidiagonal matrix is always triangular and we can also invert in $O(n)$ operations: if $U \ensuremath{\bm{\x}} = \ensuremath{\bm{\b}}$ then $x_n = b_n/u{n,n}$ and for $k = n-1,\ensuremath{\ldots},1$
\[
x_k = {b_k - u_{k,k+1} x_{k+1} \over u_{k,k}}.
\]
\textbf{Definition (Tridiagonal)} If a square matrix has bandwidths $l = u = 1$ it is \emph{tridiagonal}.

For example, 
\[
A = \begin{bmatrix} a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} & a_{2,3} \\
 & \ensuremath{\ddots} & \ensuremath{\ddots} & \ensuremath{\ddots} \\
&& a_{n-1,n-2} &                                 a_{n-1,n-1} & a_{n-1,n} \\
&&&a_{n,n-1} & a_{n,n}
\end{bmatrix}
\]
is tridiagonal. Matrix multiplication is clearly $O(n)$ operations. But so is solving linear systems.  We will see why later. 



