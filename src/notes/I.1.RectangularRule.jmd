

# Rectangular rule

One possible definition for an integral is the limit of a Riemann sum, for example:
$$
  ∫_a^b f(x) {\rm d}x = \lim_{n → ∞} h ∑_{k=1}^n f(x_k)
$$
where $x_k = a+kh$ are evenly spaced points dividing up the interval $[a,b]$, that
is  with the _step size_ $h = (b-a)/n$.
This suggests an algorithm known as the _(right-sided) rectangular rule_
for approximating an integral: choose $n$ large so that
$$
  ∫_a^b f(x) {\rm d}x ≈ h ∑_{k=1}^n f(x_k).
$$
In the lab we explore practical implementation of this approximation, and observe that
the error in approximation is bounded by $C/n$ for some constant $C$. This can be expressed using
"Big-O" notation:
$$
∫_a^b f(x) {\rm d}x = h ∑_{k=1}^n f(x_k) + O(1/n).
$$

In these notes we consider the "Analysis" part of "Numerical Analysis": we want to _prove_ the
convergence rate of the approximation, including finding an explicit expression for the constant $C$.

To tackle this question we consider the error incurred on a single "rectangle", then sum up the errors on rectangles.

Now for a secret. There are only so many tools available in analysis (especially at this stage of your career), and
one can make a safe bet that the right tool in any analysis proof is either (1) integration-by-parts, (2) geometric series
or (3) Taylor series. In this case we use (1):

**Lemma ((Right-sided) Rectangular Rule error on one panel)**
Assuming $f$ is differentiable we have
$$
∫_a^b f(x) {\rm d}x = (b-a) f(b) + δ
$$
where $|δ| ≤ M (b-a)^2$ for $M = \sup_{a ≤ x ≤ b}|f'(x)|$.

**Proof**
We write
$$
\meeq{
∫_a^b f(x) {\rm d}x = ∫_a^b (x-a)' f(x)  {\rm d}x = [(x-a) f(x)]_a^b - ∫_a^b (x-a) f'(x) {\rm d} x \ccr
= (b-a) f(b) + \underbrace{\left(-∫_a^b (x-a) f'(x) {\rm d} x \right)}_δ.
}
$$
Recall that we can bound the absolute value of an integral by the sepremum of the integrand
times the width of the integration interval:
$$
\abs{∫_a^b g(x) {\rm d} x} ≤ (b-a) \sup_{a ≤ x ≤ b}|g(x)|.
$$
The lemma thus follows since
$$
\abs{∫_a^b (x-a) f'(x) {\rm d} x} ≤ (b-a) \sup_{a ≤ x ≤ b}|(x-a) f'(x)| ≤ M (b-a)^2.
$$
∎

Now summing up the errors in each panel gives us the error of using the Rectangular rule:

**Theorem (Rectangular Rule error)**
Assuming $f$ is differentiable we have
$$
∫_a^b f(x) {\rm d}x =  h ∑_{k=1}^n f(x_k) +  δ
$$
where $|δ| ≤ M (b-a) h$ for $M = \sup_{a ≤ x ≤ b}|f'(x)|$, $h = (b-a)/n$ and $x_k = a + kh$.

**Proof**
We split the integral into a sum of smaller integrals:
$$
∫_a^b f(x) {\rm d}x = ∑_{k=1}^n  ∫_{x_{k-1}}^{x_k} f(x) {\rm d}x =
∑_{k=1}^n  \br[(x_k - x_{k-1}) f(x_k) + δ_k] =  h ∑_{k=1}^n f(x_k) +  \underbrace{∑_{k=1}^n δ_k}_δ
$$
where $δ_k$, the error on each panel as in the preceding lemma, satisfies
$$
|δ_k| ≤ (x_k-x_{k-1})^2 \sup_{x_{k-1} ≤ x ≤ x_k}|f'(x)| ≤ M h^2.
$$
Thus using the triangular inequality we have
$$
|δ| = \abs{ ∑_{k=1}^n δ_k} ≤ ∑_{k=1}^n |δ_k| ≤ M n h^2 = M(b-a)h.
$$
∎

Note a consequence of this lemma is that the approximation converges as $n → 0$. In the labs and problem
sheets we will consider the left-sided rule:
$$
∫_a^b f(x) {\rm d}x ≈  h ∑_{k=0}^{n-1} f(x_k).
$$
We also consider the _Trapezium rule_. Here we approximate an integral  by an affine function:
$$
∫_a^b f(x) {\rm d} x ≈ ∫_a^b {(b-x)f(a) + (x-a)f(b) \over b-a} \dx
= {b-a \over 2} \br[f(a) + f(b)].
$$
Subdividing an interval $a = x_0 < x_1 < … < x_n = b$ and applying this approximation separately on
each subinterval $[x_{k-1},x_k]$, where $h = (b-a)/n$ and $x_k = a + kh$, leads to the approximation
$$
∫_a^b f(x) {\rm d}x ≈  {h \over 2} f(a) + h ∑_{k=1}^{n-1} f(x_k) + {h \over 2} f(b)
$$
We shall see both experimentally and provably that this approximation converges faster than the
rectangular rule.






