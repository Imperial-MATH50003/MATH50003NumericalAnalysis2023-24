# Structured Matrices


We have seen how algebraic operations (`+`, `-`, `*`, `/`) are
defined exactly in terms of rounding ($‚äï$, $‚äñ$, $‚äó$, $‚äò$) 
for floating point numbers. Now we see how this allows us
to do (approximate) linear algebra operations on matrices. 


A matrix can be stored in different formats, in particular it is important for large scale
simulations that we take advantage of
_sparsity_: if we know a matrix has entries that are guaranteed to be zero we can implement faster algorithms.
We shall see that this comes up naturally in numerical methods for solving differential equations. 


In particular, we will discuss some basic types of structure in matrices:

1. _Dense_: This can be considered unstructured, where we need to store all entries in a vector or matrix. Matrix-vector multiplication reduces directly to standard algebraic operations. Solving linear systems with dense matrices will be discussed later.
2. _Triangular_: If a matrix is upper or lower triangular multiplication requires roughly half the number of operations. Crucially, we can apply the inverse of a triangular matrix using forward-elimination or back-substitution.
3. _Banded_: If a matrix is zero apart from entries a fixed distance from  the diagonal it is called banded and matrix-vector multiplication has a lower _complexity_: the number of operations scales linearly with the dimension (instead of quadratically).
 We discuss diagonal, tridiagonal and bidiagonal matrices.

**Remark** For those who took the first half of the module, there was an important emphasis on working with _linear operators_ rather than _matrices_. That is, there was
an emphasis on basis-independent mathematical techniques, which is critical for extension of results to infinite-dimensional spaces (which might not have a complete basis). However, in terms of practical computation we need to work with some representation of an operator and the 
most natural is a matrix. And indeed we will see in the next section how infinite-dimensional differential equations can be solved by reduction to finite-dimensional matrices. (Restricting attention to matrices is also important as some of the students have not taken the first half of the module.)

## Dense matrices



A basic operation is matrix-vector multiplication. For a field $ùîΩ$ (typically $‚Ñù$ or $‚ÑÇ$, or this can be relaxed to be a ring), consider a matrix $A ‚àà ùîΩ^{m √ó n}$ and vector $ùê± ‚àà ùîΩ^n$. Recall
the usual definition of matrix multiplication:
$$
Aùê± := \begin{bmatrix} ‚àë_{j=1}^n a_{1,j} x_j \\ ‚ãÆ \\ ‚àë_{j=1}^n a_{m,j} x_j \end{bmatrix}.
$$
When we are working with floating point numbers $A ‚àà F^{m √ó n}$ we obtain an approximation:
$$
Aùê± ‚âà \begin{bmatrix} ‚®Å_{j=1}^n (a_{1,j}  ‚äó x_j) \\ ‚ãÆ \\  ‚®Å_{j=1}^n (a_{m,j}  ‚äó x_j) \end{bmatrix}.
$$
This actually encodes an algorithm for computing the entries. And this algorithm uses $O(m n)$ floating point operations (FLOPs): each of the $m$ entry consists
of $n$ multiplications and $n-1$ additions, hence we $2n-1 = O(n)$ per row for a total of $m(2n-1) = O(mn)$ operations. In the problem sheet we see how
the floating point error can be bounded in terms of norms, thus reducing the problem to a purely mathematical concept.


Sometimes there are multiple ways of implementing numerical algorithms. We have an alternative formula where we multiple by columns:
if we write
$$
A = \begin{bmatrix} ùêö_1 | ‚ãØ | ùêö_n \end{bmatrix}
$$
where $ùêö_j = A ùêû_j ‚àà ùîΩ^m$ being the columns of $A$ we also have
$$
A ùê± = x_1 ùêö_1  + ‚ãØ + x_n ùêö_n.
$$
The floating point formula for this is exactly the same as the previous algorithm and the number of operations is the same. Just the order of operations has changed. 
Suprisingly, this latter version is significantly faster.


## Triangular matrices

The simplest sparsity case is being triangular: where all entries above or below the diagonal are zero. 
Matrix multiplication can be modified to take advantage of this. Eg., if $U ‚àà ùîΩ^{n √ó n}$ is upper triangular we have:
$$
Uùê± = \begin{bmatrix} ‚àë_{j=1}^n a_{1,j} x_j \\ ‚àë_{j=2}^n a_{2,j} x_j  \\ ‚ãÆ \\ ‚àë_{j=n}^n a_{m,j} x_j \end{bmatrix}.
$$
When implemented in floating point this uses roughly half the number of multiplications: $n + (n-1) + ‚Ä¶ + 1 = n(n+1)/2$ multiplications.
The complexity is still $O(n^2)$ however. 

Triangularity allows us to also invert systems using forward- or back-substitution. In particular if $ùê±$ solves $U ùê± = ùêõ$ then we have:
$$
x_k = {b_k - ‚àë_{j=k+1}^n u_{kj} x_j \over u_{kk}}
$$
Thus we can compute $x_n, x_{n-1},‚Ä¶,x_1$ in sequence (using floating point operations).



## Banded matrices

A _banded matrix_ is zero off a prescribed number of diagonals. 
We call the number of (potentially) non-zero diagonals the _bandwidths_:


**Definition (bandwidths)** A matrix $A$ has _lower-bandwidth_ $l$ if 
$A[k,j] = 0$ for all $k-j > l$ and _upper-bandwidth_ $u$ if
$A[k,j] = 0$ for all $j-k > u$. We say that it has _strictly lower-bandwidth_ $l$
if it has lower-bandwidth $l$ and there exists a $j$ such that $A[j+l,j] \neq 0$.
We say that it has _strictly upper-bandwidth_ $u$
if it has upper-bandwidth $u$ and there exists a $k$ such that $A[k,k+u] \neq 0$.


A banded matrix has better complexity for matrix multiplication and solving linear systems:  we can multiple in $O(\min(m,n))$ operations.
We consider two cases in particular (in addition to diagonal): bidiagonal and tridiagonal. 


**Definition (Bidiagonal)** If a square matrix has bandwidths $(l,u) = (1,0)$ it is _lower-bidiagonal_ and
if it has bandwidths $(l,u) = (0,1)$ it is _upper-bidiagonal_. 

For example, if
$$
U = \begin{bmatrix} u_{1,1} & u_{1,2} \\ & ‚ã± & ‚ã± \\
&&                                 u_{n-1,n-1} & u_{n-1,n} \\
&&& u_{n,n}
\end{bmatrix}
$$
is upper-bidiagonal multiplication becomes
$$
Uùê± = \begin{bmatrix} u_{1,1} x_1 + u_{1,2} x_2 \\ u_{2,2} x_2 + u_{2,3} x_3  \\ ‚ãÆ \\ u_{n-1,n-1} x_{n-1} + u_{n-1,n} x_n \\u_{n,n} x_n \end{bmatrix}.
$$
This requires only $O(n)$ operations. A bidiagonal matrix is always triangular and we can also invert in $O(n)$ operations: if $U ùê± = ùêõ$ then $x_n = b_n/u{n,n}$
and for $k = n-1,‚Ä¶,1$
$$
x_k = {b_k - u_{k,k+1} x_{k+1} \over u_{k,k}}.
$$


**Definition (Tridiagonal)** If a square matrix has bandwidths $l = u = 1$ it is _tridiagonal_.


For example, 
$$
A = \begin{bmatrix} a_{1,1} & a_{1,2} \\
a_{2,1} & a_{2,2} & a_{2,3} \\
 & ‚ã± & ‚ã± & ‚ã± \\
&& a_{n-1,n-2} &                                 a_{n-1,n-1} & a_{n-1,n} \\
&&&a_{n,n-1} & a_{n,n}
\end{bmatrix}
$$
is tridiagonal. Matrix multiplication is clearly $O(n)$ operations. But so is solving linear systems. 
We will see why later. 