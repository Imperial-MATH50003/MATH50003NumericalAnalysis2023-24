# # MATH50003 (2022â€“23)
# # Lab 6: III.3 Cholesky Factorisation and III.4 Polynomial Regression

# In this lab we explore using LU, PLU and Cholesky factorisations, and
# implement algorithms for computing a Cholesky factorisation.

# **Learning Outcomes**
#
# Mathematical knowledge:
#
# 1. Cholesky and reverse Cholesky factorisations, including for banded matrices.
# 2. Vandermonde matrices and least squares.
#
# Coding knowledge:
#
# 1. Using the `lu` and `cholesky` functions.
# 2. Solving least squares problems via `\`

# We load the following packages

using LinearAlgebra, Plots, BenchmarkTools, Test

# ### III.3 LU and Cholesky Factorisations

# LU, PLU and Cholesky factorisations are closely related
# matrix factorisations that reduce a square matrix to a product of
# lower and upper triangular matrices and possibly with a permutation.
# We will only focus on the practical usage of LU and PLU, whilst digging into the
# details of implementing and generalising a Cholesky factorisation.

# 

# ### III.3.1 LU Factorisation


# If $A âˆˆ ğ”½^{n Ã— n}$ is a square matrix where $ğ”½$ is a field ($â„$ or $â„‚$)
# then we can find lower and upper triangular matrices $L,U âˆˆ ğ”½^{n Ã— n}$ such that 
# $$
# A = LU.
# $$
# This is equivalent to Gaussian elimination but we will only focus on practical usage.
# This factorisation can be computed using the `lu` function, but as the default is a PLU factorisation we add a flag
# telling it not to use pivoting/permutations:

A = [1.0 1 1;
     2   4 8;
     1   4 9]

L,U = lu(A, NoPivot()) # No Pivot is needed to tell lu not to using permutations

# This matches what we derived by hand and indeed:

@test A â‰ˆ L*U

# We can use an LU factorisation to reduce solving a linear system to inverting triangular matrices:

b = randn(3)
c = L \ b # computed using forward elimination (even though L is a Matrix, \ detects it is lower triangular)
x = U \ c # computed using back substitution
@test A \ b == x # matches Julia's \ exactly, since this matrix did not need pivoting

# If a matrix has a zero on a pivot we know by equivalence to Gaussian elimination that an LU factorisation
# does not exist:

A[1,1] = 0
lu(A, NoPivot()) # throws an error

# But even if it has a very small but non-zero entry we get huge errors:

A[1,1] = 1E-14
L,U = lu(A, NoPivot()) # Succeeds but suddenly U is on order of 2E14!

norm(A \ b - U\(L\b)) # Very large error! A \ b uses pivoting now.

# **WARNING** The parantheses are important: algebra is left-associatitive so had we written `U\L\b` this would have been interpreted as
# `(L\U) \ b` which would have meant `inv(inv(L)*U)*b == U \ (L*b).


# **Problem 1** For `A` defined above, consider setting the `A[1,1] = Îµ` for `Îµ = 10.0 ^ (-k)` for `k = 0,â€¦,14`
# with the right-hand side `b = [1,2,3]`.
# Use a scale the $y$-axis logarithmically to conjecture the growth rate in the error of using LU compared to `\` as $k â†’ âˆ$. 
# Hint: you can either allocate a vector of errors that is populated in a for-loop or write a simple comprehension.

## TODO: Do a log-log plot for A with its 1,1 entry set to different Îµ and guess the growth rate.
## SOLUTION

A = [1.0 1 1;
     2   4 8;
     1   4 9]

b = [1,2,3]


n = 15
errs = zeros(n)
for k = 1:n
    A[1,1] = 10.0 ^ (1-k)
    L,U = lu(A, NoPivot())
    errs[k] = norm(A\b - L \ (U \ b))
end

scatter(0:n-1, errs; yscale=:log10, yticks = 10.0 .^ (0:30), xticks = 1:15)
## The error grows exponentially, like 10^(2k)

## END


# **Problem 2(a)** Consider the Helmholtz equations
# $$
# \begin{align*}
# u(0) &= 0 \\
# u(1) &= 0 \\
# u'' + k^2 u &= {\rm e}^x
# \end{align*}
# $$
# discretised with finite-differences to result in a tridiagonal system.
# Use the `lu` function without pivoting to
# compute the LU factorization of the tridiagonal matrix. What sparsity structure
# do you observe in `L` and `U`? Does this structure depend on $n$ or $k$?

## TODO: Apply lu to the discretisation for Helmholtz derived in the last lab and investigate its structure.

## SOLUTION


##Â We make a function that returns the Helmholtz matrix:
function helmholtz(n, k)
    x = range(0, 1; length = n + 1)
    h = step(x)
    Tridiagonal([fill(1/h^2, n-1); 0], 
                    [1; fill(k^2-2/h^2, n-1); 1], 
                    [0; fill(1/h^2, n-1)])
end

lu(helmholtz(20, 2), NoPivot()) # L is lower bidiagonal and U is lower bidiagonal, regardless of n or k
## END


# ## III.3.2 PLU Factorisation

# In general it is necessary to do pivoting, a feature you have seen
# in Gaussian elimination but as Problem 1 demonstrates we need to do so even if we do not encounter
# a zero. This corresponds to a factorisation of the form
# $$
#  A = P^âŠ¤LU
# $$
# where $P$ is a permutation matrix, $L$ is lower triangular and $U$ is upper triangular.
# We compute this as follows, printing out the permutation:

A = [0.1 1 1;
     2   4 8;
     1   4 9]

L,U,Ïƒ = lu(A)
Ïƒ

# The permutation is encoded as a vector $Ïƒ$. More precisely, we have
# $$
#     P^âŠ¤ ğ¯ = ğ¯[Ïƒ]
# $$
# Thus we can solve a linear system as follows: we first permute the entries of the right-hand side:

b = [10,11,12]
bÌƒ = b[Ïƒ] # permute the entries to [b[2],b[3],b[1]]

# We now solve as before:
c = L \ bÌƒ # invert L with forward elimination
x = U \ c # invert U with back substitution

@test x == A \ b # \ also use PLU to do the solve so these exactly equal

# **Problem 2(b)** Repeat Problem 2(a) but with a PLU decomposition. 
# Are $L$ and $U$ still banded?

## TODO:

## SOLUTION
lu(helmholtz(20, 2)).L # L is no longer banded: its penultimate row is dense
## END

# ## III.3.3 Cholesky Factorisation




# 4. Timings and Stability

The different factorisations have trade-offs between speed and stability.
First we compare the speed of the different factorisations on a symmetric positive
definite matrix, from fastest to slowest:

```julia
n = 100
A = Symmetric(rand(n,n)) + 100I # shift by 10 ensures positivity
@btime cholesky(A);
@btime lu(A);
@btime qr(A);
```
On my machine, `cholesky` is ~1.5x faster than `lu`,  
which is ~2x faster than QR. 



In terms of stability, QR computed with Householder reflections
(and Cholesky for positive definite matrices) are stable, 
whereas LU is usually unstable (unless the matrix
is diagonally dominant). PLU is a very complicated story: in theory it is unstable,
but the set of matrices for which it is unstable is extremely small, so small one does not
normally run into them.

Here is an example matrix that is in this set. 
```julia
function badmatrix(n)
    A = Matrix(1I, n, n)
    A[:,end] .= 1
    for j = 1:n-1
        A[j+1:end,j] .= -1
    end
    A
end
A =1badmatrix(5)
```
Note that pivoting will not occur (we do not pivot as the entries below the diagonal are the same magnitude as the diagonal), thus the PLU Factorisation is equivalent to an LU factorisation:
```julia
L,U = lu(A)
```
But here we see an issue: the last column of `U` is growing exponentially fast! Thus when `n` is large
we get very large errors:
```julia
n = 100
b = randn(n)
A = badmatrix(n)
norm(A\b - qr(A)\b) # A \ b still uses lu
```
Note `qr` is completely fine:
```julia
norm(qr(A)\b - qr(big.(A)) \b) # roughly machine precision
```

Amazingly, PLU is fine if applied to a small perturbation of `A`:
```julia
Îµ = 0.000001
AÎµ = A .+ Îµ .* randn.()
norm(AÎµ \ b - qr(AÎµ) \ b) # Now it matches!
```



The big _open problem_ in numerical linear algebra is to prove that the set of matrices
for which PLU fails has extremely small measure.




    Note hidden in this proof is a simple algorithm form computing the Cholesky factorisation.

**Algorithm 3 (Cholesky)** 
```julia
function mycholesky(A)
    T = eltype(A)
    n,m = size(A)
    if n â‰  m
        error("Matrix must be square")
    end
    if A â‰  A'
        error("Matrix must be symmetric")
    end
    T = eltype(A)
    L = LowerTriangular(zeros(T,n,n))
    Aâ±¼ = copy(A)
    for j = 1:n
        Î±,ğ¯ = Aâ±¼[1,1],Aâ±¼[2:end,1]
        if Î± â‰¤ 0
            error("Matrix is not SPD")
        end 
        L[j,j] = sqrt(Î±)
        L[j+1:end,j] = ğ¯/sqrt(Î±)

        # induction part
        Aâ±¼ = Aâ±¼[2:end,2:end] - ğ¯*ğ¯'/Î±
    end
    L
end

A = Symmetric(rand(100,100) + 100I)
L = mycholesky(A)
@test A â‰ˆ L*L'
```


This algorithm succeeds if and only if $A$ is symmetric positive definite.



# We now consider a Cholesky factorisation for tridiagonal matrices. Since we are assuming the
# matrix is symmetric, we will use a special type `SymTridiagonal` that captures the symmetry.
# In particular, `SymTridiagonal(dv, eu) == Tridiagonal(ev, dv, ev)`.

# **Problem 3** Complete the following
# implementation of `mycholesky` to return a `Bidiagonal` cholesky factor in $O(n)$ operations.


## return a Bidiagonal L such that L'L == A (up to machine precision)
## You are allowed to change A
function mycholesky(A::SymTridiagonal)
    d = A.dv # diagonal entries of A
    u = A.ev # sub/super-diagonal entries of A
    T = float(eltype(A)) # return type, make float in case A has Ints
    n = length(d)
    ld = zeros(T, n) # diagonal entries of L
    ll = zeros(T, n-1) # sub-diagonal entries of L

    ##Â TODO: populate the diagonal entries ld and the sub-diagonal entries ll
    ## of L so that L*L' â‰ˆ A
    ## SOLUTION
    ld[1] = sqrt(d[1])
    for k = 1:n-1
        ll[k] = u[k]/ld[k]
        ld[k+1] = sqrt(d[k+1]-ll[k]^2)
    end
    ## END

    Bidiagonal(ld, ll, :L)
end

n = 1000
A = SymTridiagonal(2*ones(n),-ones(n-1))
L = mycholesky(A)
@test L*L' â‰ˆ A


# ## III.4 Polynomial Interpolation and Regression

# ### III.4.1 Polynomial Interpolation

# Thus a quick-and-dirty way to to do interpolation is to invert the Vandermonde matrix
# (which we saw in the least squares setting with more samples then coefficients):

using Plots, LinearAlgebra
f = x -> cos(10x)
n = 5

x = range(0, 1; length=n+1)# evenly spaced points (BAD for interpolation)
V = x .^ (0:n)' # Vandermonde matrix
c = V \ f.(x) # coefficients of interpolatory polynomial
p = x -> dot(c, x .^ (0:n))

g = range(0,1; length=1000) # plotting grid
plot(g, f.(g); label="function")
plot!(g, p.(g); label="interpolation")
scatter!(x, f.(x); label="samples")



# When $m = n$ a least squares fit by a polynomial becomes _interpolation_:
# the approximating polynomial will fit the data exactly. That is, for
# $$
# p(x) = âˆ‘_{k = 0}^n p_k x^k
# $$
# and $x_1, â€¦, x_n âˆˆ â„$, we choose $p_k$ so that $p(x_j) = f(x_j)$ for
# $j = 1, â€¦, n$. 

# **Problem 1.1** Complete the following function which returns a rectangular _Vandermonde matrix_:
# a matrix $V âˆˆ â„^{m Ã— n}$ such that
# $$
# V * \begin{bmatrix} p_0\\ â‹® \\p_n \end{bmatrix} = \begin{bmatrix} p(x_1)\\ â‹® \\p(x_m) \end{bmatrix}
# $$

function vandermonde(ğ±, n) # ğ± = [x_1,â€¦,x_m]
    m = length(ğ±)
    ## TODO: Make V
    ## SOLUTION
    ## There are also solutions using broadcasting or for loops.
    ##Â e.g. 
    ## ğ± .^ (0:n-1)'
    [ğ±[j]^k for j = 1:m, k = 0:n-1]
    ## END
end

n = 1000
ğ± = range(0, 0.5; length=n)
V = vandermonde(ğ±, n) # square Vandermonde matrix
## if all coefficients are 1 then p(x) = (1-x^n)/(1-x)
@test V * ones(n) â‰ˆ (1 .- ğ± .^ n) ./ (1 .- ğ±)


# Inverting the square Vandermonde matrix is a way of computing coefficients from function
# samples. That is, solving
# $$
# Vğœ = \begin{bmatrix} f(x_1) \\ â‹® \\ f(x_n) \end{bmatrix}
# $$
# Gives the coefficients of a polynomial $p(x)$ so that $p(x_j) = f(x_j)$.
# Whether an interpolation is actually close to a function is a subtle question,
# involving properties of the function, distribution of the sample points $x_1,â€¦,x_n$,
# and round-off error.
# A classic example is:
# $$
#   f_M(x) = {1 \over M x^2 + 1}
# $$
# where the choice of $M$ can dictate whether interpolation at evenly spaced points converges.

# **Problem 1.2** Interpolate $1/(4x^2+1)$ and $1/(25x^2 + 1)$ at an evenly spaced grid of $n$
# points, plotting the solution at a grid of $1000$ points. For $n = 50$ does your interpolation match
# the true function?  Does increasing $n$ to 400 improve the accuracy? How about using `BigFloat`?

n = 50
ğ± = range(-1, 1; length=n)
ğ  = range(-1, 1; length=1000) # plotting grid

## TODO: interpolate 1/(10x^2 + 1) and 1/(25x^2 + 1) at $ğ±$, plotting both solutions evaluated at
## the grid ğ . Hint: use a rectangular Vandermonde matrix to evaluate your polynomial on ğ . Remember
## `plot(ğ±, ğŸ)` will create a new plot whilst `plot!(ğ±, ğŸ)` will add to an existing plot.

## SOLUTION
V = vandermonde(ğ±, n)
V_g = vandermonde(ğ , n)
f_4 = x -> 1/(4x^2 + 1)
ğœ_4 = V \ f_4.(ğ±)
f_25 = x -> 1/(25x^2 + 1)
ğœ_25 = V \ f_25.(ğ±)

plot(ğ , V_g*ğœ_4; ylims=(-1,1))
plot!(ğ , V_g*ğœ_25)
## We see large errors near Â±1 for both examples. 
## END
#
## TODO: repeat the experiment with `n = 400` and observe what has changed.
## SOLUTION
n = 400
ğ± = range(-1, 1; length=n)
ğ  = range(-1, 1; length=1000) # plotting grid

V = vandermonde(ğ±, n)
V_g = vandermonde(ğ , n)
f_4 = x -> 1/(4x^2 + 1)
ğœ_4 = V \ f_4.(ğ±)
f_25 = x -> 1/(25x^2 + 1)
ğœ_25 = V \ f_25.(ğ±)

plot(ğ , V_g*ğœ_4; ylims=(-1,1))
plot!(ğ , V_g*ğœ_25)
## Still does not converge
##Â END
#
## TODO: repeat the experiment with `n = 400` and using `BigFloat` and observe what has changed.
## Hint: make sure to make your `range` be `BigFloat` valued, e.g., by using `big`.
## SOLUTION
n = 400
ğ± = range(big(-1), 1; length=n)
ğ  = range(big(-1), 1; length=1000) # plotting grid

V = vandermonde(ğ±, n)
V_g = vandermonde(ğ , n)
f_4 = x -> 1/(4x^2 + 1)
ğœ_4 = V \ f_4.(ğ±)
f_25 = x -> 1/(25x^2 + 1)
ğœ_25 = V \ f_25.(ğ±)

plot(ğ , V_g*ğœ_4; ylims=(-1,1))
plot!(ğ , V_g*ğœ_25)
## With M = 4 it looks like it now is converging. This suggests the issue before was numerical error.
## For M = 25 the solution is even less accurate, which suggests the issue is a lack of mathematical
## convergence.

## END


**Example 1 (quadratic fit)** Suppose we want to fit noisy data by a quadratic
$$
p(x) = pâ‚€ + pâ‚ x + pâ‚‚ x^2
$$
That is, we want to choose $pâ‚€,pâ‚,pâ‚‚$ at data samples $x_1, â€¦, x_m$ so that the following is true:
$$
pâ‚€ + pâ‚ x_k + pâ‚‚ x_k^2 â‰ˆ f_k
$$
where $f_k$ are given by data. We can reinterpret this as a least squares problem: minimise the norm
$$
\left\| \begin{bmatrix} 1 & x_1 & x_1^2 \\ â‹® & â‹® & â‹® \\ 1 & x_m & x_m^2 \end{bmatrix}
\begin{bmatrix} pâ‚€ \\ pâ‚ \\ pâ‚‚ \end{bmatrix} - \begin{bmatrix} f_1 \\ â‹® \\ f_m \end{bmatrix} \right \|
$$
We can solve this using the QR decomposition:
```julia
m,n = 100,3

x = range(0,1; length=m) # 100 points
f = 2 .+ x .+ 2x.^2 .+ 0.1 .* randn.() # Noisy quadratic

A = x .^ (0:2)'  # 100 x 3 matrix, equivalent to [ones(m) x x.^2]
Q,RÌ‚ = qr(A)
QÌ‚ = Q[:,1:n] # Q represents full orthogonal matrix so we take first 3 columns

pâ‚€,pâ‚,pâ‚‚ = RÌ‚ \ QÌ‚'f
```
We can visualise the fit:
```julia
p = x -> pâ‚€ + pâ‚*x + pâ‚‚*x^2

scatter(x, f; label="samples", legend=:bottomright)
plot!(x, p.(x); label="quadratic")
```
Note that `\` with a rectangular system does least squares by default:
```julia
A \ f
```



# **Problem 1.3** Repeat the previous problem but now using _least squares_: instead of interpolating,
# use least squares on a large grid: choose the coefficients of a degree $(n-1)$ polynomial so that
# $$
#     \left\| \begin{bmatrix} p(x_1) \\ â‹® \\ p(x_m) \end{bmatrix} - \begin{bmatrix} f(x_1) \\ â‹® \\ f(x_m) \end{bmatrix} \right \|.
# $$
# is minimised.
# Does this improve the convergence properties? Do you think convergence for a least squares approximation
# is dictated by the radius of convergence of the corresponding Taylor series?
# Hint: use the rectangular Vandermonde matrix to setup the Least squares system.

n = 50 # use basis [1,x,â€¦,x^(49)]
ğ± = range(-1, 1; length=500) # least squares grid
ğ  = range(-1, 1; length=2000) # plotting grid

## TODO: interpolate 1/(10x^2 + 1) and 1/(25x^2 + 1) at $ğ±$, plotting both solutions evaluated at
## the grid ğ . Hint: use a rectangular Vandermonde matrix to evaluate your polynomial on ğ . Remember
## `plot(ğ±, ğŸ)` will create a new plot whilst `plot!(ğ±, ğŸ)` will add to an existing plot.

## SOLUTION
V = vandermonde(ğ±, n)
V_g = vandermonde(ğ , n)
f_4 = x -> 1/(4x^2 + 1)
ğœ_4 = V \ f_4.(ğ±)
f_25 = x -> 1/(25x^2 + 1)
ğœ_25 = V \ f_25.(ğ±)

plot(ğ , V_g*ğœ_4; ylims=(-1,1))
plot!(ğ , V_g*ğœ_25)

## Yes, now both approximations appear to be converging.
## This is despite the radius of convergence of both functions being
## smaller than the interval of interpolation.

## END
